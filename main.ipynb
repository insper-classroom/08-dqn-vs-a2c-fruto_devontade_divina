{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO, DQN, A2C\n",
    "# import gymnasium as gym\n",
    "# from stable_baselines3.common.logger import configure\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up logging\n",
    "# tmp_path = \"./results/cartpole\"\n",
    "# new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "# # Create environment and model\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# model = A2C(\n",
    "#     policy=\"MlpPolicy\",\n",
    "#     env=env,\n",
    "#     learning_rate=0.0007,  \n",
    "#     n_steps=10,            \n",
    "#     gamma=0.99,            \n",
    "#     gae_lambda=1.0,      \n",
    "#     ent_coef=0.0,         \n",
    "#     vf_coef=0.5,           \n",
    "#     max_grad_norm=0.5,     \n",
    "#     device='cpu'\n",
    "# )\n",
    "\n",
    "# # Set logger and train\n",
    "# model.set_logger(new_logger)\n",
    "# model.learn(total_timesteps=500_000)\n",
    "\n",
    "# # Evaluate the trained model\n",
    "# mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "# print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')\n",
    "\n",
    "# # Plotting the training curve\n",
    "# log_data = pd.read_csv(f\"{tmp_path}/progress.csv\")  # Load the CSV log\n",
    "# sns.set_theme(style=\"darkgrid\")  # Set Seaborn style\n",
    "# plt.figure(figsize=(10, 6))  # Set figure size\n",
    "\n",
    "# # Plot the reward curve with smoothing\n",
    "# sns.lineplot(x=\"time/total_timesteps\", y=\"rollout/ep_rew_mean\", data=log_data, label=\"Mean Episode Reward\")\n",
    "# plt.xlabel(\"Timesteps\")\n",
    "# plt.ylabel(\"Mean Episode Reward\")\n",
    "# plt.title(\"A2C Training Curve on CartPole-v1\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "# # Render the trained model\n",
    "# print('modelo treinado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'log_interval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Log every 1000 timesteps\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# DQN Model (runs first)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model_dqn \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_update_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexploration_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexploration_initial_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexploration_final_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This enables logging\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable tensorboard to avoid duplicate logging\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set same log interval for both algorithms\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train DQN\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model_dqn\u001b[38;5;241m.\u001b[39mset_logger(dqn_logger)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'log_interval'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN, A2C\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create models/ directory\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Set up logging paths (CSV only)\n",
    "a2c_log_path = \"./results/cartpole_a2c\"\n",
    "dqn_log_path = \"./results/cartpole_dqn\"\n",
    "a2c_logger = configure(a2c_log_path, [\"csv\"])\n",
    "dqn_logger = configure(dqn_log_path, [\"csv\"])\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Set logging interval\n",
    "log_interval = 1000  # Log every 1000 timesteps\n",
    "\n",
    "# DQN Model (runs first)\n",
    "model_dqn = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=0.001,\n",
    "    buffer_size=50000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    target_update_interval=500,\n",
    "    exploration_fraction=0.2,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    learning_starts=1000,\n",
    "    device='cpu',\n",
    "    verbose=1,  # Enable progress output\n",
    "    tensorboard_log=None  # Disable tensorboard\n",
    ")\n",
    "\n",
    "# Train DQN\n",
    "model_dqn.set_logger(dqn_logger)\n",
    "print(\"Training DQN...\")\n",
    "model_dqn.learn(total_timesteps=50000, log_interval=log_interval)\n",
    "model_dqn.save(\"models/dqn_cartpole\")\n",
    "\n",
    "# Evaluate DQN\n",
    "mean_reward_dqn, std_reward_dqn = evaluate_policy(model_dqn, model_dqn.get_env(), n_eval_episodes=10)\n",
    "print(f'DQN Mean reward: {mean_reward_dqn} +/- {std_reward_dqn:.2f}')\n",
    "\n",
    "# A2C Model (runs second)\n",
    "model_a2c = A2C(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=0.0007,\n",
    "    n_steps=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    device='cpu',\n",
    "    verbose=1,  # Enable progress output\n",
    "    tensorboard_log=None  # Disable tensorboard\n",
    ")\n",
    "\n",
    "# Train A2C\n",
    "model_a2c.set_logger(a2c_logger)\n",
    "print(\"Training A2C...\")\n",
    "model_a2c.learn(total_timesteps=50000, log_interval=log_interval)\n",
    "model_a2c.save(\"models/a2c_cartpole\")\n",
    "\n",
    "# Evaluate A2C\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(model_a2c, model_a2c.get_env(), n_eval_episodes=10)\n",
    "print(f'A2C Mean reward: {mean_reward_a2c} +/- {std_reward_a2c:.2f}')\n",
    "\n",
    "# Load log data\n",
    "a2c_log_data = pd.read_csv(f\"{a2c_log_path}/progress.csv\")\n",
    "dqn_log_data = pd.read_csv(f\"{dqn_log_path}/progress.csv\")\n",
    "\n",
    "# Add algorithm identifier (not strictly needed for separate plots but kept for consistency)\n",
    "a2c_log_data['Algorithm'] = 'A2C'\n",
    "dqn_log_data['Algorithm'] = 'DQN'\n",
    "\n",
    "# Set up plotting with two subplots\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot DQN on top subplot\n",
    "sns.lineplot(x=\"time/total_timesteps\", y=\"rollout/ep_rew_mean\", data=dqn_log_data, ax=ax1, color=\"blue\")\n",
    "ax1.set_title(\"DQN Training Curve on CartPole-v1\")\n",
    "ax1.set_xlabel(\"\")  # Remove x-label for top plot (shared with bottom)\n",
    "ax1.set_ylabel(\"Mean Episode Reward\")\n",
    "\n",
    "# Plot A2C on bottom subplot\n",
    "sns.lineplot(x=\"time/total_timesteps\", y=\"rollout/ep_rew_mean\", data=a2c_log_data, ax=ax2, color=\"orange\")\n",
    "ax2.set_title(\"A2C Training Curve on CartPole-v1\")\n",
    "ax2.set_xlabel(\"Timesteps\")\n",
    "ax2.set_ylabel(\"Mean Episode Reward\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
